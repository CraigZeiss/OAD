{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Install czmodel and dependencies (only needed once)\n",
    "#! pip install --upgrade pip\n",
    "#! pip install czmodel\n",
    "\n",
    "# install some other useful packages\n",
    "\n",
    "#!pip install sklearn\n",
    "#!pip install livelossplot --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# this can be used to switch on/off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train Simple U-Net for segmentation and use it inside ZEN\n",
    "\n",
    "This notebook illustrates a **simple** workflow of training an ANN with [TensorFlow 2](https://www.tensorflow.org/) using the keras API and exporting the trained model to the [czmodel format](https://pypi.org/project/czmodel/) to be ready for use within the [Intellesis](https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html) software inside ZEN.\n",
    "\n",
    "* **The trained model is for demo purposes and trained on a small dataset.**\n",
    "* **Therefore, this notebook is meant to be understood as a guide for exporting trained models**\n",
    "* **The notebook does not show how train a model correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# imports to train a simple TF2 + Keras model for segmentation\n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from traintools.model import build_model\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "# Optional: suppress TF warnings\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "print(tf.version.GIT_VERSION, tf.__version__)\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline\n",
    "This section describes a **simple** training procedure that creates a trained Keras model - in this case a U-Net.\n",
    "\n",
    "* Therefore, the pipelin only represents one possible custom training procedure.\n",
    "* Such procedure will vary from case to case and will contain more sophisticated ways to generate an optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameters for loading the training data\n",
    "\n",
    "# adjust to jour needs\n",
    "base_directory = r\"c:\\Users\\m1srh\\OneDrive - Carl Zeiss AG\\Smart_Microscopy_Workshop\\datasets\"\n",
    "\n",
    "# place the original and label *.png images here\n",
    "imgfolder = \"nucleus_data/images/\"\n",
    "#imgfolder = \"mouse_embryo/images\"\n",
    "\n",
    "# masks images have one channel (0=background and 1=object)\n",
    "maskfolder = 'nucleus_data/label/'\n",
    "#maskfolder = \"mouse_embryo/label\"\n",
    "\n",
    "# construct the final path to the images and labels\n",
    "IMAGES_FOLDER = os.path.join(base_directory, imgfolder)\n",
    "MASKS_FOLDER = os.path.join(base_directory, maskfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define the number of channels inside the image to be segmented\n",
    "CHANNELS = 1\n",
    "\n",
    "# define bit depth for images  and masks\n",
    "BITDEPTH_IMAGES = tf.dtypes.uint16\n",
    "BITDEPTH_MASKS = tf.dtypes.uint16\n",
    "\n",
    "# the model contains 2 classes, eg. object and background\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# tilesize for training which depends on the GPU memory size etc.\n",
    "SZTR = 512\n",
    "\n",
    "# Model and Training Parameters\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT = 0.15\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCHSIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define very simple augumentation function - can be extented as needed\n",
    "def random_crop_and_pad_image_and_label(image, label, size):\n",
    "    \"\"\"Randomly crops image together with labels.\n",
    "\n",
    "    Args:\n",
    "      image: A Tensor with shape [D_1, ..., D_K, N]\n",
    "      label: A Tensor with shape [D_1, ..., D_K, M]\n",
    "      size: A Tensor with shape [K] indicating the crop size.\n",
    "    Returns:\n",
    "      A tuple of (cropped_image, cropped_label).\n",
    "    \"\"\"\n",
    "    \n",
    "    combined = tf.concat([image, label], axis=-1)\n",
    "    image_shape = tf.shape(image)\n",
    "    combined_pad = tf.image.pad_to_bounding_box(combined, 0, 0,\n",
    "                                                tf.maximum(size[0], image_shape[0]),\n",
    "                                                tf.maximum(size[1], image_shape[1]))\n",
    "    \n",
    "    last_label_dim = tf.shape(label)[-1]\n",
    "    last_image_dim = tf.shape(image)[-1]\n",
    "    \n",
    "    combined_crop = tf.image.random_crop(combined_pad,\n",
    "                                         size=tf.concat([size, [last_label_dim + last_image_dim]],\n",
    "                                                        axis=0))\n",
    "    \n",
    "    return (combined_crop[:, :, :last_image_dim], combined_crop[:, :, last_image_dim:])\n",
    "\n",
    "# helper function to reorder labels incase of multiple classes\n",
    "def reorder_labels(mask):\n",
    "    for i, unique_value in enumerate(np.unique(mask)):\n",
    "        mask[mask == unique_value] = i\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This part contains the logic to read pairs of images and label masks for training.\n",
    "\n",
    "# get the sample images\n",
    "sample_images = sorted([os.path.join(IMAGES_FOLDER, f) for f in os.listdir(IMAGES_FOLDER)\n",
    "                        if os.path.isfile(os.path.join(IMAGES_FOLDER, f))])\n",
    "\n",
    "# get the maks\n",
    "sample_masks = sorted([os.path.join(MASKS_FOLDER, f) for f in os.listdir(MASKS_FOLDER)\n",
    "                       if os.path.isfile(os.path.join(MASKS_FOLDER, f))])\n",
    "\n",
    "# load images as numpy arrays\n",
    "images_loaded = np.asarray([tf.image.decode_image(tf.io.read_file(sample_path),\n",
    "                                                  dtype=BITDEPTH_IMAGES,\n",
    "                                                  channels=CHANNELS).numpy()\n",
    "                            for sample_path in sample_images]).astype(np.float32)\n",
    "\n",
    "masks_loaded = [tf.image.decode_image(tf.io.read_file(sample_path), dtype=BITDEPTH_MASKS).numpy()[...,0] for sample_path in sample_masks]\n",
    "\n",
    "# load masks and create one_hot encoded masks\n",
    "masks_loaded = [tf.convert_to_tensor(reorder_labels(mask_loaded).astype(np.uint8)) for mask_loaded in masks_loaded]\n",
    "masks_loaded = np.asarray([tf.one_hot(mask_loaded, depth=NUM_CLASSES).numpy() for mask_loaded in masks_loaded])\n",
    "\n",
    "# Remark: For details see [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot)\n",
    "# tf.one_hot creates X channels from X labels: 1 => [0.0, 1.0], 0 => [1.0, 0.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the datasets into training, test and validation sets\n",
    "def split_datasets(images, shuffel=False, testsize=0.2):\n",
    "    \n",
    "    # split into train and test set\n",
    "    train, test = train_test_split(images, shuffle=False, test_size=testsize)\n",
    "    \n",
    "    # split training set into final training set and validation set\n",
    "    train, val = train_test_split(train, shuffle=False, test_size=testsize)\n",
    "    \n",
    "    return train, test, val\n",
    "\n",
    "# do the splitting\n",
    "images_train, images_test, images_val = split_datasets(images_loaded, testsize=0.2)\n",
    "masks_train, masks_test, masks_val = split_datasets(masks_loaded, testsize=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# check if the images fit to the labels - always a good idea ... :-)\n",
    "\n",
    "#pair2show = len(images_loaded)\n",
    "pair2show = 3\n",
    "\n",
    "for i in range(pair2show):\n",
    "    \n",
    "    # cut out a smaller image for visualization\n",
    "    cropped_image, cropped_mask = random_crop_and_pad_image_and_label(image=images_loaded[i, ...],\n",
    "                                                                      label=masks_loaded[i, ...],\n",
    "                                                                      size=[SZTR, SZTR])\n",
    "  \n",
    "    # show the images next to the labels\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(cropped_image[...,0])\n",
    "    ax2.imshow(cropped_mask[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create a tfdatasets from the images and masks\n",
    "tfdataset = tf.data.Dataset.from_tensor_slices((images_loaded, masks_loaded))\n",
    "tfdataset_train = tf.data.Dataset.from_tensor_slices((images_train, masks_train))\n",
    "tfdataset_test = tf.data.Dataset.from_tensor_slices((images_test, masks_test))\n",
    "tfdataset_val = tf.data.Dataset.from_tensor_slices((images_val, masks_val))\n",
    "\n",
    "# apply the augmentation function to the tfdataset = just a random crop and create batch datasets \n",
    "\n",
    "# complete set\n",
    "tfdataset = tfdataset.map(lambda image, label: random_crop_and_pad_image_and_label(image,\n",
    "                                                                                   label,\n",
    "                                                                                   size=[SZTR, SZTR])).batch(BATCHSIZE)\n",
    "\n",
    "# train set\n",
    "tfdataset_train = tfdataset_train.map(lambda image, label: random_crop_and_pad_image_and_label(image,\n",
    "                                                                                               label,\n",
    "                                                                                               size=[SZTR, SZTR])).batch(BATCHSIZE)\n",
    "# test set\n",
    "tfdataset_test = tfdataset_test.map(lambda image, label: random_crop_and_pad_image_and_label(image,\n",
    "                                                                                             label,\n",
    "                                                                                             size=[SZTR, SZTR])).batch(BATCHSIZE)\n",
    "# validation set\n",
    "tfdataset_val = tfdataset_val.map(lambda image, label: random_crop_and_pad_image_and_label(image,\n",
    "                                                                                           label,\n",
    "                                                                                           size=[SZTR, SZTR])).batch(BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate th U-Net model\n",
    "model = build_model(start_filters=16,\n",
    "                    numchannels=CHANNELS,\n",
    "                    numclasses=NUM_CLASSES,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    perimagestd=False,\n",
    "                    kernelsize=KERNEL_SIZE,\n",
    "                    dropout=DROPOUT\n",
    "                   )\n",
    "\n",
    "# sho model summary (optional)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fit the model to the loaded data\n",
    "This part fits the model to the loaded data and evaluates it on the training data. In this test example we do not care about an actual evaluation of the model using validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train the U-Net model\n",
    "\n",
    "# define number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# fit the model to the data using the \"training\" set and for validataion the \"validation\" set\n",
    "model.fit(tfdataset_train,\n",
    "          callbacks=[PlotLossesKerasTF()],\n",
    "          validation_data=(tfdataset_val),\n",
    "          epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the loss and accuary values\n",
    "loss, accuracy = model.evaluate(tfdataset_test)\n",
    "\n",
    "# show the final accuracy achieved on the \"test\" set which was never used for \"anything\" so far\n",
    "print(\"The model achieves {}% accuracy on the test data.\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create an iterator for the dataset\n",
    "dataset_it = iter(tfdataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get an image and a mask from the dataset\n",
    "for img, mask in dataset_it:\n",
    "\n",
    "    #run the prediction and get the masks using argmax\n",
    "    pred = model.predict(img)[0]\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "    # scale the image and shwo the results - just re-execute the cell to display a different image\n",
    "    img_scaled = img.numpy()[0] / img.numpy().max()\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    # show images and set titles\n",
    "    ax1.imshow(img_scaled[...,0])\n",
    "    ax2.imshow(pred, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax3.imshow(mask[0,...,0])\n",
    "\n",
    "    ax1.set_title('Image')\n",
    "    ax2.set_title('Prediction')\n",
    "    ax3.set_title('Label Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a CZModel from the trained Keras model\n",
    "\n",
    "In this section we export the trained model to the CZModel format using the czmodel library and some additional meta data all possible parameter choices are described in the [ANN model specification](https://pypi.org/project/czmodel/) that can be found on the PyPi packager for `czmodel`.\n",
    "\n",
    "### Define Meta-Data\n",
    "\n",
    "We first define the meta-data needed to run the model within the Intellesis infrastructure. The `czmodel` package offers a named tuple `ModelMetadata` that allows to either parse as JSON file as described or to directly specify the parameters as shown below.\n",
    "\n",
    "### Create a Model Specification Object\n",
    "\n",
    "The export functions provided by the `czmodel` package expect a `ModelSpec` tuple that features the Keras model to be exported and the corresponding model metadata.\n",
    "\n",
    "Therefore, we wrap our model and the `model_metadata` instance into a `ModelSpec` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# those functions are provided by the PyPi package called czmodel (by ZEISS)\n",
    "from czmodel.util.preprocessing import add_preprocessing_layers\n",
    "from czmodel.model_metadata import ModelMetadata, ModelSpec\n",
    "from czmodel import convert_from_model_spec, convert_from_json_spec\n",
    "\n",
    "# Define the model metadata\n",
    "\n",
    "model_name = 'Simple_Nuclei_SegmentationModel'\n",
    "model_classes = [\"Background\", \"Nuclei\"]\n",
    "\n",
    "#model_name = 'Embryo_DIC_simple'\n",
    "#model_classes = [\"Background\", \"Embryo\"]\n",
    "\n",
    "pixtype = 'Gray16'\n",
    "bordersize = 64\n",
    "\n",
    "model_metadata = ModelMetadata.from_params(name=model_name, \n",
    "                                           color_handling='ConvertToMonochrome',\n",
    "                                           pixel_type=pixtype,\n",
    "                                           classes=model_classes,\n",
    "                                           border_size=bordersize)\n",
    "\n",
    "\n",
    "# Create a model specification object used for conversion\n",
    "model_spec = ModelSpec(model=model, model_metadata=model_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perform model export into *.czmodel file format\n",
    "\n",
    "The `czmodel` library offers two functions to perform the actual export. \n",
    "\n",
    "* `convert_from_json_spec` allows to provide a JSON file with all information to convert a model in SavedModel format on disk to a `.czmodel` file that can be loaded with ZEN.\n",
    "* `convert_from_model_spec` expects a `ModelSpec` object, an output path and name and optionally target spatial dimensions for the expected input of the exported model. From this information it creates a `.czmodel` file containing the specified model.\n",
    "\n",
    "```python\n",
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path=folder_to_store_czmodel, \n",
    "                        output_name=name_of_the_model, \n",
    "                        spatial_dims=spatial_dims)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define dimensions - ZEN Intellesis requires fully defined spatial dimensions.\n",
    "# This is the tile size used by the ZEN TilingStrategy to pass an image to the SegmentationService.\n",
    "\n",
    "# Define target spatial dimensions of the model for inference.\n",
    "spatial_dims = 1024, 1024\n",
    "\n",
    "# package and save as CZMODEL - for inference the U-Net model will be exported as ONNX\n",
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path='./czmodel_output', \n",
    "                        output_name=model_name, \n",
    "                        spatial_dims=spatial_dims)\n",
    "\n",
    "# In the example above there will be a \"\"./czmodel_output/simple_nuclei_segmodel.czmodel\" file saved on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remarks\n",
    "The generated .czmodel file can be directly loaded into ZEN Intellesis to perform segmentation tasks with the trained model.\n",
    "\n",
    "If there is already a trained model in SavedModel format present on disk, it can also be converted\n",
    "by providing a meta-data JSON file as described in the [ANN Model Specification](https://pypi.org/project/czmodel/).\n",
    "\n",
    "The following JSON document describes the same meta-data applied in the use case above:\n",
    "\n",
    "```json\n",
    "{\n",
    "\"BorderSize\": 64,\n",
    "\"ColorHandling\": \"ConvertToMonochrome\",\n",
    "\"PixelType\": \"Gray16\",\n",
    "\"Classes\": [\"Background\", \"Nuclei\"],\n",
    "\"ModelPath\": \"saved_tf2_model_output\",\n",
    "}\n",
    "```\n",
    "\n",
    "This information can be copied to a file e.g. in the current working directory `./model_spec.json`\n",
    "that also contains the trained model in SavedModel format e.g. generated by the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# save the trained TF2.SavedModel as a folder structure\n",
    "# The folder + the JSON file can be also used to import the model in ZEN\n",
    "\n",
    "model.save('./saved_tf2_model_output_dims_unset/') # save as TF.SavedModel without dims set\n",
    "\n",
    "# add extra layer infront to define dimensions and save as TF2.SavedModel\n",
    "add_preprocessing_layers(model, layers=None, spatial_dims=spatial_dims).save('./saved_tf2_model_output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example of a model XML description**\n",
    "\n",
    "<img src=\"mdpics/model_xml.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To import the newly created model just use the **`Import`** function of the Intellesis Trainable Segmentation module in ZEN.\n",
    "\n",
    "<img src=\"mdpics/zen_import_model1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Select the **`simple_nuclei_segmodel.czmodel`** file and press the **`Open`** button.\n",
    "\n",
    "<img src=\"mdpics/zen_import_model2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the IP-function **`Segmentation`** to segment an image using the imported CZMODEL (containing the trained network).\n",
    "\n",
    "<img src=\"mdpics/zen_import_model_IPseg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use the trained model to analyse an image there are two main options\n",
    "\n",
    "1. directly create an Image Analysis Setting based on the model (no class hierarchy, but very simple)\n",
    "2. assign the trained model to s specific class inside a customized image analysis setting (shown below)\n",
    "\n",
    "The crucial step (when not using option 1) is the Select the correct **`Class Segmentation Method`** inside the Image Analysis Wizard.\n",
    "\n",
    "<img src=\"mdpics/zen_import_model_IA1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the **`Select Model`** function to assign the trained model and the actual **class** (from the trained model) of interest to assign the model / class to the respective object inside the image analysis setting.\n",
    "\n",
    "<img src=\"mdpics/zen_import_model_IA2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the trained model will be used to segment the image. The built-in ZEN Tiling Client automatically  to chunk the image and deal with complex dimensions, like Use the **`Scenes`** etc.\n",
    "\n",
    "Additional Post-Processing options, incl. a Minimum Confidence Threshold can be applied to further refine the results.\n",
    "\n",
    "<img src=\"mdpics/zen_import_model_IA3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, the model can be loaded into ZEN by using the **Import** function on the **JSON file**. \n",
    "\n",
    "If the model is supposed to be provided to other parties it is usually easier to exchange .czmodel files instead of SavedModel directories with corresponding JSON meta-data files.\n",
    "\n",
    "The `czmodel` library also provides a `convert_from_json_spec` function that accepts the above mentioned JSON file and creates a CZModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This is an additional way how to create a CZMODEL from a saved TF2 model on disk + JSON file.\n",
    "# The currently recommended way to to create the CZMODEL directly by using czmodel.convert_from_model_spec\n",
    "# the path to the TF2.SavedModel folder is defined in the JSON shown above\n",
    "\n",
    "convert_from_json_spec(model_spec_path='model_spec_dims_unset.json',\n",
    "                       output_path='model_from_json',\n",
    "                       output_name = 'simple_nuclei_segmodel_from_json',\n",
    "                       spatial_dims=spatial_dims)\n",
    "\n",
    "convert_from_json_spec(model_spec_path='model_spec.json',\n",
    "                       output_path='model_from_json_dims_unset',\n",
    "                       output_name = 'simple_nuclei_segmodel_from_json',\n",
    "                       spatial_dims=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the commands below from a terminal to present the notebook as a slideshow.\n",
    "\n",
    "`\n",
    "jupyter nbconvert Train_UNet_Export_as_CZMODEL_final.ipynb --to slides --post serve \n",
    "    --SlidesExporter.reveal_theme=serif \n",
    "    --SlidesExporter.reveal_scroll=True \n",
    "    --SlidesExporter.reveal_transition=none\n",
    "`\n",
    "\n",
    "Or install the [RISE Extension](https://rise.readthedocs.io/en/stable/) to display a slideshow directly from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "jupyter nbconvert Train_UNet_Export_as_CZMODEL_final.ipynb --to slides --post serve --SlidesExporter.reveal_theme=serif --SlidesExporter.reveal_scroll=True --SlidesExporter.reveal_transition=none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:czmodel]",
   "language": "python",
   "name": "conda-env-czmodel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
